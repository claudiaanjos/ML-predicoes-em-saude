# Aula 10

### **Materiais**

* [Slides](https://edisciplinas.usp.br/pluginfile.php/7418118/mod_resource/content/1/Aula%2015.pdf) utilizados na aula.

* [Artigo](https://arxiv.org/abs/2010.09337) referência da aula: *Interpretable machine learning - a brief history, state-of-the-art and challenges*.

### **Aula**

Na aula, abordamos a distinção entre importância preditiva e causalidade, destacando razões para analisar ou, em alguns casos, evitar a análise da importância preditora de variáveis. Exploramos diferentes aspectos da interpretação, incluindo interpretação intrínseca e extrínseca, análise de componentes, sensibilidade à permutação de preditores, aproximações locais e modelos substitutos.

Foi ressaltado que o objetivo principal da predição em machine learning não é compreender a causalidade de um fenômeno, mas sim predizê-lo. A interpretação em machine learning não equivale à causalidade; em vez disso, é uma maneira de compreender como o algoritmo realizou a predição. Além disso, discutimos técnicas específicas, como SHAP values (Shapley Additive Explanations), LIME, incerteza estatística da predição e dependência de variáveis, como meios de aprimorar a interpretação dos resultados preditivos sem assumir causalidade direta.

### **Artigo**

O artigo aborda o tema de Machine Learning (ML) interpretável, que se refere à capacidade de compreender e explicar as decisões tomadas pelos modelos de ML. Destacam-se três estratégias principais para alcançar a interpretabilidade: a observação direta dos componentes do modelo, a análise do efeito de perturbações nos preditores e a avaliação de aproximações locais.

Além disso, o artigo destaca a crescente aceitação da interpretabilidade na comunidade científica de ML. Essa aceitação evidencia o reconhecimento da importância de compreender e explicar as decisões dos modelos de ML para promover a confiança e a aplicação responsável dessas tecnologias.
